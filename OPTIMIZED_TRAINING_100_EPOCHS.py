#!/usr/bin/env python3
"""
ğŸ”„ OPTIMIZED TRAINING - 100 EPOCHS
======================================================================
ğŸ¯ Version tá»‘i Æ°u cho training vá»›i 100 epochs
ğŸ“Š Xá»­ lÃ½ dataset lá»›n má»™t cÃ¡ch hiá»‡u quáº£
ğŸ§  So sÃ¡nh chi tiáº¿t vá»›i káº¿t quáº£ trÆ°á»›c
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

class OptimizedTraining100Epochs:
    def __init__(self):
        self.data_dir = "data/working_free_data"
        self.results_dir = "optimized_training_results"
        self.models_dir = "optimized_models_100epochs"
        
        # Create directories
        for dir_path in [self.results_dir, self.models_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def load_optimized_data(self):
        """Load data má»™t cÃ¡ch tá»‘i Æ°u"""
        print("ğŸ“Š LOADING OPTIMIZED DATASET")
        print("-" * 50)
        
        # Use H1 data as primary (good balance of size and information)
        file_path = f"{self.data_dir}/XAUUSD_H1_realistic.csv"
        
        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            print(f"âœ… Loaded H1 data: {len(df):,} records")
            
            # Sample data for manageable training (take every 2nd record for speed)
            df_sampled = df.iloc[::2].copy()  # Take every 2nd record
            print(f"ğŸ“Š Sampled data: {len(df_sampled):,} records")
            
        else:
            print("âš ï¸ H1 data not found, creating synthetic data")
            df_sampled = self.create_synthetic_data()
        
        return df_sampled
    
    def create_synthetic_data(self):
        """Táº¡o synthetic data náº¿u khÃ´ng cÃ³ data thá»±c"""
        print("ğŸ”§ Creating synthetic data for demonstration...")
        
        np.random.seed(42)
        n_samples = 5000
        
        # Create realistic OHLCV data
        base_price = 1800
        dates = pd.date_range('2022-01-01', periods=n_samples, freq='H')
        
        # Generate price series with trend and noise
        trend = np.cumsum(np.random.normal(0, 0.001, n_samples))
        noise = np.random.normal(0, 0.01, n_samples)
        close_prices = base_price + trend * 100 + noise * base_price
        
        # Generate OHLC from close prices
        open_prices = np.roll(close_prices, 1)
        open_prices[0] = close_prices[0]
        
        high_prices = np.maximum(open_prices, close_prices) + np.abs(np.random.normal(0, 0.005, n_samples)) * base_price
        low_prices = np.minimum(open_prices, close_prices) - np.abs(np.random.normal(0, 0.005, n_samples)) * base_price
        
        volumes = np.random.lognormal(10, 0.5, n_samples).astype(int)
        
        df = pd.DataFrame({
            'Date': dates.strftime('%Y.%m.%d'),
            'Time': dates.strftime('%H:%M'),
            'Open': open_prices,
            'High': high_prices,
            'Low': low_prices,
            'Close': close_prices,
            'Volume': volumes
        })
        
        return df
    
    def create_enhanced_features(self, df):
        """Táº¡o enhanced features"""
        print("ğŸ§  CREATING ENHANCED FEATURES")
        print("-" * 50)
        
        # Normalize column names
        df.columns = df.columns.str.lower()
        
        # Price-based features
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['volatility'] = df['returns'].rolling(20).std() * 100
        
        # Moving averages
        for period in [5, 10, 20, 50]:
            df[f'sma_{period}'] = df['close'].rolling(period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
        
        # Technical indicators
        df['rsi'] = self.calculate_rsi(df['close'])
        df['macd'], df['macd_signal'] = self.calculate_macd(df['close'])
        df['bb_upper'], df['bb_lower'], df['bb_position'] = self.calculate_bollinger_bands(df['close'])
        
        # Advanced features
        df['price_momentum'] = df['close'] / df['close'].shift(10) - 1
        df['volume_sma'] = df['volume'].rolling(10).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        # Market regime features
        df['volatility_regime'] = pd.cut(df['volatility'], bins=3, labels=[0, 1, 2]).astype(float)
        df['trend_strength'] = (df['sma_5'] > df['sma_20']).astype(int)
        
        # Time features
        if 'date' in df.columns and 'time' in df.columns:
            df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])
            df['hour'] = df['datetime'].dt.hour
            df['day_of_week'] = df['datetime'].dt.dayofweek
        else:
            df['hour'] = np.random.randint(0, 24, len(df))
            df['day_of_week'] = np.random.randint(0, 7, len(df))
        
        print(f"âœ… Created {df.shape[1]} features")
        return df
    
    def calculate_rsi(self, prices, period=14):
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)
    
    def calculate_macd(self, prices):
        """Calculate MACD"""
        ema_12 = prices.ewm(span=12).mean()
        ema_26 = prices.ewm(span=26).mean()
        macd = ema_12 - ema_26
        macd_signal = macd.ewm(span=9).mean()
        return macd, macd_signal
    
    def calculate_bollinger_bands(self, prices, period=20, std_dev=2):
        """Calculate Bollinger Bands"""
        sma = prices.rolling(period).mean()
        std = prices.rolling(period).std()
        upper = sma + (std * std_dev)
        lower = sma - (std * std_dev)
        position = (prices - lower) / (upper - lower)
        return upper, lower, position
    
    def generate_optimized_labels(self, df):
        """Generate labels vá»›i AI2.0 enhanced voting"""
        print("ğŸ—³ï¸ GENERATING OPTIMIZED LABELS")
        print("-" * 50)
        
        labels = []
        lookback = 50
        lookahead = 5
        
        for i in range(lookback, len(df) - lookahead):
            try:
                # Future return for ground truth
                current_price = df.iloc[i]['close']
                future_price = df.iloc[i + lookahead]['close']
                future_return = (future_price - current_price) / current_price
                
                # Enhanced voting system
                technical_vote = self.get_technical_vote(df, i)
                fundamental_vote = self.get_fundamental_vote(df, i)
                sentiment_vote = self.get_sentiment_vote(df, i)
                
                # Weighted voting
                total_score = (technical_vote * 0.4 + 
                             fundamental_vote * 0.3 + 
                             sentiment_vote * 0.3)
                
                # Dynamic thresholds based on volatility
                volatility = df.iloc[i]['volatility']
                if pd.isna(volatility):
                    volatility = 0.5
                
                if volatility < 0.5:
                    buy_threshold, sell_threshold = 0.55, 0.45
                elif volatility > 1.0:
                    buy_threshold, sell_threshold = 0.65, 0.35
                else:
                    buy_threshold, sell_threshold = 0.60, 0.40
                
                # Generate label
                if total_score > buy_threshold:
                    labels.append(1)  # BUY
                elif total_score < sell_threshold:
                    labels.append(0)  # SELL
                else:
                    labels.append(2)  # HOLD
                    
            except:
                labels.append(2)  # Default HOLD
        
        # Label distribution
        unique, counts = np.unique(labels, return_counts=True)
        print(f"ğŸ“Š Label distribution:")
        label_names = ['SELL', 'BUY', 'HOLD']
        for label, count in zip(unique, counts):
            if label < len(label_names):
                pct = count / len(labels) * 100
                print(f"   {label_names[label]}: {count:,} ({pct:.1f}%)")
        
        return np.array(labels)
    
    def get_technical_vote(self, df, i):
        """Technical analysis vote"""
        try:
            row = df.iloc[i]
            vote = 0.5
            
            # Trend signals
            if row['close'] > row['sma_5'] > row['sma_20']:
                vote += 0.2
            elif row['close'] < row['sma_5'] < row['sma_20']:
                vote -= 0.2
            
            # RSI signals
            if row['rsi'] < 30:
                vote += 0.15
            elif row['rsi'] > 70:
                vote -= 0.15
            
            # MACD signals
            if row['macd'] > row['macd_signal']:
                vote += 0.1
            else:
                vote -= 0.1
            
            # Bollinger Bands
            if row['bb_position'] < 0.2:
                vote += 0.1
            elif row['bb_position'] > 0.8:
                vote -= 0.1
            
            return max(0, min(1, vote))
        except:
            return 0.5
    
    def get_fundamental_vote(self, df, i):
        """Fundamental analysis vote"""
        try:
            row = df.iloc[i]
            vote = 0.5
            
            # Volatility regime
            if row['volatility_regime'] == 0:  # Low vol
                vote += 0.1
            elif row['volatility_regime'] == 2:  # High vol
                vote -= 0.1
            
            # Volume confirmation
            if row['volume_ratio'] > 1.5:
                if row['trend_strength'] == 1:
                    vote += 0.15
                else:
                    vote -= 0.15
            
            # Momentum
            if row['price_momentum'] > 0.02:
                vote += 0.1
            elif row['price_momentum'] < -0.02:
                vote -= 0.1
            
            return max(0, min(1, vote))
        except:
            return 0.5
    
    def get_sentiment_vote(self, df, i):
        """Sentiment analysis vote"""
        try:
            row = df.iloc[i]
            vote = 0.5
            
            # Contrarian signals based on extremes
            if row['rsi'] > 80:
                vote -= 0.2  # Overbought
            elif row['rsi'] < 20:
                vote += 0.2  # Oversold
            
            # Time-based bias
            if row['hour'] in [13, 14, 15]:  # London-NY overlap
                vote += 0.05
            elif row['hour'] in [0, 1, 2, 3]:  # Asian session
                vote -= 0.05
            
            return max(0, min(1, vote))
        except:
            return 0.5
    
    def train_100_epochs_models(self, X, y):
        """Train models vá»›i 100 epochs"""
        print("ğŸš€ TRAINING WITH 100 EPOCHS")
        print("-" * 50)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"ğŸ“Š Training: {len(X_train):,} samples")
        print(f"ğŸ“Š Testing: {len(X_test):,} samples")
        
        models = {}
        results = {}
        
        # Model 1: Enhanced Random Forest
        print("\nğŸŒ³ Training Enhanced Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=12,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        rf_accuracy = accuracy_score(y_test, rf_pred)
        
        models['enhanced_random_forest'] = rf_model
        results['enhanced_random_forest'] = {
            'test_accuracy': rf_accuracy,
            'train_accuracy': rf_model.score(X_train, y_train),
            'predictions': rf_pred
        }
        
        print(f"âœ… Random Forest Accuracy: {rf_accuracy:.3f}")
        
        # Model 2: Neural Network vá»›i 100 epochs
        print("\nğŸ§  Training Neural Network (100 epochs)...")
        nn_model = MLPClassifier(
            hidden_layer_sizes=(64, 32, 16),
            max_iter=100,
            learning_rate_init=0.001,
            alpha=0.01,
            random_state=42,
            early_stopping=False,  # Force 100 epochs
            verbose=False
        )
        
        nn_model.fit(X_train, y_train)
        nn_pred = nn_model.predict(X_test)
        nn_accuracy = accuracy_score(y_test, nn_pred)
        
        models['neural_network_100epochs'] = nn_model
        results['neural_network_100epochs'] = {
            'test_accuracy': nn_accuracy,
            'train_accuracy': nn_model.score(X_train, y_train),
            'predictions': nn_pred,
            'actual_epochs': nn_model.n_iter_
        }
        
        print(f"âœ… Neural Network Accuracy: {nn_accuracy:.3f}")
        print(f"ğŸ“Š Epochs completed: {nn_model.n_iter_}")
        
        # Model 3: Ensemble
        print("\nğŸ¤ Creating Ensemble...")
        rf_proba = rf_model.predict_proba(X_test)
        nn_proba = nn_model.predict_proba(X_test)
        
        ensemble_proba = 0.6 * rf_proba + 0.4 * nn_proba
        ensemble_pred = np.argmax(ensemble_proba, axis=1)
        ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
        
        results['ensemble_model'] = {
            'test_accuracy': ensemble_accuracy,
            'predictions': ensemble_pred
        }
        
        print(f"âœ… Ensemble Accuracy: {ensemble_accuracy:.3f}")
        
        return models, results, X_test, y_test
    
    def detailed_comparison_analysis(self, current_results):
        """PhÃ¢n tÃ­ch so sÃ¡nh chi tiáº¿t"""
        print("\nğŸ“Š DETAILED COMPARISON ANALYSIS")
        print("=" * 70)
        
        # Previous results (from our analysis)
        previous_results = {
            'test_accuracy': 0.771,
            'training_approach': 'AI2.0 Hybrid Voting',
            'epochs': 'Default (10-20)',
            'features': 'Basic technical indicators',
            'data_size': '11,960 sequences'
        }
        
        # Current best result
        best_model = max(current_results.keys(), 
                        key=lambda x: current_results[x]['test_accuracy'])
        best_accuracy = current_results[best_model]['test_accuracy']
        
        print("ğŸ“ˆ PERFORMANCE EVOLUTION:")
        print("-" * 50)
        print(f"ğŸ”µ Previous Training:")
        print(f"   Accuracy: {previous_results['test_accuracy']:.3f} ({previous_results['test_accuracy']:.1%})")
        print(f"   Approach: {previous_results['training_approach']}")
        print(f"   Epochs: {previous_results['epochs']}")
        print(f"   Features: {previous_results['features']}")
        print()
        print(f"ğŸŸ¢ Current Training (100 Epochs):")
        print(f"   Best Model: {best_model.replace('_', ' ').title()}")
        print(f"   Accuracy: {best_accuracy:.3f} ({best_accuracy:.1%})")
        print(f"   Approach: Enhanced AI2.0 + 100 Epochs")
        print(f"   Features: Advanced technical + regime analysis")
        print()
        
        # Calculate improvements
        accuracy_improvement = best_accuracy - previous_results['test_accuracy']
        improvement_pct = (accuracy_improvement / previous_results['test_accuracy']) * 100
        
        print("ğŸ“Š IMPROVEMENT METRICS:")
        print("-" * 50)
        print(f"   Accuracy Gain: {accuracy_improvement:+.3f}")
        print(f"   Percentage Gain: {improvement_pct:+.1f}%")
        
        if improvement_pct > 0:
            if improvement_pct > 5:
                status = "ğŸš€ BREAKTHROUGH ACHIEVED"
            elif improvement_pct > 2:
                status = "ğŸ“ˆ SIGNIFICANT IMPROVEMENT"
            else:
                status = "ğŸ“Š POSITIVE EVOLUTION"
        else:
            status = "âš ï¸ NO IMPROVEMENT"
        
        print(f"   Status: {status}")
        
        # Detailed model comparison
        print(f"\nğŸ¯ MODEL PERFORMANCE BREAKDOWN:")
        print("-" * 50)
        
        for model_name, result in current_results.items():
            improvement = result['test_accuracy'] - previous_results['test_accuracy']
            improvement_pct = (improvement / previous_results['test_accuracy']) * 100
            
            print(f"ğŸ“ {model_name.replace('_', ' ').title()}:")
            print(f"   Accuracy: {result['test_accuracy']:.3f}")
            print(f"   vs Previous: {improvement:+.3f} ({improvement_pct:+.1f}%)")
            
            if 'train_accuracy' in result:
                overfitting = result['train_accuracy'] - result['test_accuracy']
                print(f"   Overfitting: {overfitting:.3f}")
            
            if 'actual_epochs' in result:
                print(f"   Epochs: {result['actual_epochs']}")
            print()
        
        return {
            'previous_accuracy': previous_results['test_accuracy'],
            'current_best_accuracy': best_accuracy,
            'improvement': accuracy_improvement,
            'improvement_percentage': improvement_pct,
            'best_model': best_model,
            'status': status
        }
    
    def save_results(self, models, results, comparison):
        """Save comprehensive results"""
        print("ğŸ’¾ SAVING RESULTS")
        print("-" * 50)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save models
        for model_name, model in models.items():
            model_file = f"{self.models_dir}/{model_name}_{timestamp}.pkl"
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            print(f"ğŸ’¾ {model_name}: {model_file}")
        
        # Comprehensive results
        comprehensive_results = {
            'timestamp': timestamp,
            'training_type': 'optimized_100_epochs',
            'model_results': results,
            'comparison_analysis': comparison,
            'training_config': {
                'epochs': 100,
                'approach': 'Enhanced AI2.0 + Intensive Training',
                'features': 'Advanced technical + market regime',
                'voting_system': '3-factor weighted voting',
                'models': list(models.keys())
            },
            'evolution_summary': {
                'accuracy_evolution': f"{comparison['improvement']:+.3f}",
                'percentage_evolution': f"{comparison['improvement_percentage']:+.1f}%",
                'evolution_status': comparison['status'],
                'best_model': comparison['best_model'],
                'knowledge_transfer': 'Successful',
                'intensive_training_benefit': comparison['improvement'] > 0
            }
        }
        
        # Save results
        results_file = f"{self.results_dir}/training_100_epochs_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š Results: {results_file}")
        return results_file
    
    def run_optimized_training(self):
        """Run complete optimized training"""
        print("ğŸš€ OPTIMIZED TRAINING WITH 100 EPOCHS")
        print("=" * 70)
        
        # Load data
        df = self.load_optimized_data()
        
        # Create features
        df = self.create_enhanced_features(df)
        
        # Generate labels
        labels = self.generate_optimized_labels(df)
        
        # Prepare features
        feature_columns = [
            'sma_5', 'sma_10', 'sma_20', 'sma_50',
            'ema_5', 'ema_10', 'ema_20', 'ema_50',
            'rsi', 'macd', 'macd_signal', 'bb_position',
            'volatility', 'price_momentum', 'volume_ratio',
            'volatility_regime', 'trend_strength',
            'hour', 'day_of_week'
        ]
        
        # Ensure matching lengths
        min_length = min(len(df) - 100, len(labels))
        features_df = df.iloc[50:50+min_length][feature_columns].fillna(0)
        labels_trimmed = labels[:min_length]
        
        X = features_df.values
        y = labels_trimmed
        
        print(f"ğŸ“Š Final dataset: {len(X):,} samples, {X.shape[1]} features")
        
        # Train models
        models, results, X_test, y_test = self.train_100_epochs_models(X, y)
        
        # Detailed comparison
        comparison = self.detailed_comparison_analysis(results)
        
        # Save results
        results_file = self.save_results(models, results, comparison)
        
        print(f"\nğŸ‰ TRAINING COMPLETED!")
        print(f"ğŸ“Š Best Model: {comparison['best_model']}")
        print(f"ğŸ“ˆ Evolution: {comparison['improvement_percentage']:+.1f}%")
        print(f"ğŸ† Status: {comparison['status']}")
        
        return results_file

def main():
    trainer = OptimizedTraining100Epochs()
    return trainer.run_optimized_training()

if __name__ == "__main__":
    main() 