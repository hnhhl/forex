#!/usr/bin/env python3
"""
GPU DIAGNOSTIC - Kiá»ƒm tra 4 váº¥n Ä‘á» chÃ­nh
"""
import sys
import warnings
warnings.filterwarnings('ignore')

print("ğŸ” GPU DIAGNOSTIC - KIá»‚M TRA 4 Váº¤N Äá»€ CHÃNH")
print("="*60)

# 1. KIá»‚M TRA MÃ”I TRÆ¯á»œNG VÃ€ XUNG Äá»˜T
print("\n1ï¸âƒ£ KIá»‚M TRA MÃ”I TRÆ¯á»œNG:")
try:
    import torch
    print(f"   âœ… PyTorch: {torch.__version__}")
    print(f"   âœ… CUDA compiled: {torch.version.cuda}")
    print(f"   âœ… CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   âœ… GPU count: {torch.cuda.device_count()}")
        print(f"   âœ… Current device: {torch.cuda.current_device()}")
        print(f"   âœ… GPU name: {torch.cuda.get_device_name(0)}")
        
        # Check CUDA version compatibility
        print(f"   âœ… CUDA runtime: {torch.version.cuda}")
        device_props = torch.cuda.get_device_properties(0)
        print(f"   âœ… GPU memory: {device_props.total_memory // 1024**3}GB")
        print(f"   âœ… Compute capability: {device_props.major}.{device_props.minor}")
    else:
        print("   âŒ CUDA not available - CHECK DRIVERS!")
        sys.exit(1)
        
except ImportError as e:
    print(f"   âŒ PyTorch import error: {e}")
    sys.exit(1)

# 2. KIá»‚M TRA MODEL CHUYá»‚N SANG GPU
print("\n2ï¸âƒ£ KIá»‚M TRA MODEL TRÃŠN GPU:")
try:
    import torch.nn as nn
    
    # Táº¡o model Ä‘Æ¡n giáº£n
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 1)
            
        def forward(self, x):
            return self.linear(x)
    
    model = TestModel()
    print(f"   ğŸ“ Model ban Ä‘áº§u: {next(model.parameters()).device}")
    
    # Chuyá»ƒn model sang GPU
    model = model.cuda()
    print(f"   ğŸ”¥ Model sau .cuda(): {next(model.parameters()).device}")
    
    # Kiá»ƒm tra táº¥t cáº£ parameters
    all_on_gpu = all(p.device.type == 'cuda' for p in model.parameters())
    print(f"   âœ… Táº¥t cáº£ parameters trÃªn GPU: {all_on_gpu}")
    
except Exception as e:
    print(f"   âŒ Model GPU error: {e}")

# 3. KIá»‚M TRA Dá»® LIá»†U TRÃŠN GPU
print("\n3ï¸âƒ£ KIá»‚M TRA Dá»® LIá»†U TRÃŠN GPU:")
try:
    # Táº¡o dá»¯ liá»‡u test
    x_cpu = torch.randn(100, 10)
    y_cpu = torch.randn(100, 1)
    
    print(f"   ğŸ“ Data ban Ä‘áº§u: x={x_cpu.device}, y={y_cpu.device}")
    
    # Chuyá»ƒn data sang GPU
    x_gpu = x_cpu.cuda()
    y_gpu = y_cpu.cuda()
    
    print(f"   ğŸ”¥ Data sau .cuda(): x={x_gpu.device}, y={y_gpu.device}")
    
    # Test forward pass
    output = model(x_gpu)
    print(f"   ğŸ”¥ Model output device: {output.device}")
    
    # Kiá»ƒm tra memory usage
    print(f"   ğŸ“Š GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
    
except Exception as e:
    print(f"   âŒ Data GPU error: {e}")

# 4. KIá»‚M TRA TENSOR VÃ€ LOSS FUNCTION Äá»’NG Bá»˜
print("\n4ï¸âƒ£ KIá»‚M TRA Äá»’NG Bá»˜ THIáº¾T Bá»Š:")
try:
    # Test loss function
    criterion = nn.MSELoss()
    
    # Táº¡o target trÃªn GPU
    target = torch.randn(100, 1).cuda()
    
    print(f"   ğŸ“ Loss function device: CPU (default)")
    print(f"   ğŸ“ Output device: {output.device}")
    print(f"   ğŸ“ Target device: {target.device}")
    
    # TÃ­nh loss
    loss = criterion(output, target)
    print(f"   ğŸ”¥ Loss device: {loss.device}")
    print(f"   âœ… Loss value: {loss.item():.4f}")
    
    # Test backward pass
    loss.backward()
    print(f"   âœ… Backward pass: SUCCESS")
    
    # Kiá»ƒm tra gradients
    grad_devices = [p.grad.device if p.grad is not None else 'None' for p in model.parameters()]
    print(f"   ğŸ”¥ Gradient devices: {grad_devices}")
    
except Exception as e:
    print(f"   âŒ Sync error: {e}")

# 5. TEST TRAINING LOOP HOÃ€N CHá»ˆNH
print("\nğŸš€ TEST TRAINING LOOP HOÃ€N CHá»ˆNH:")
try:
    import torch.optim as optim
    
    # Reset model
    model = TestModel().cuda()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    # Training data
    x_train = torch.randn(1000, 10).cuda()
    y_train = torch.randn(1000, 1).cuda()
    
    print(f"   ğŸ“Š Training data: {x_train.shape} on {x_train.device}")
    
    # Mini training loop
    model.train()
    total_loss = 0
    
    for i in range(0, len(x_train), 100):  # Batch size 100
        batch_x = x_train[i:i+100]
        batch_y = y_train[i:i+100]
        
        optimizer.zero_grad()
        output = model(batch_x)
        loss = criterion(output, batch_y)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if i == 0:  # First batch
            print(f"   ğŸ”¥ First batch - Input: {batch_x.device}, Output: {output.device}, Loss: {loss.device}")
    
    avg_loss = total_loss / (len(x_train) // 100)
    print(f"   âœ… Training loop SUCCESS - Avg Loss: {avg_loss:.4f}")
    print(f"   ğŸ”¥ Final GPU memory: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
    
except Exception as e:
    print(f"   âŒ Training loop error: {e}")

print("\nğŸ‰ DIAGNOSTIC COMPLETED!")
print("="*60)

# KIá»‚M TRA CUá»I CÃ™NG
print("\nğŸ“‹ TÃ“M Táº®T:")
if torch.cuda.is_available():
    print("âœ… GPU environment: OK")
    print("âœ… Model to GPU: OK") 
    print("âœ… Data to GPU: OK")
    print("âœ… Device sync: OK")
    print("âœ… Training loop: OK")
    print("\nğŸ”¥ GPU READY FOR TRAINING!")
else:
    print("âŒ GPU not available") 