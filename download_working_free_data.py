#!/usr/bin/env python3
"""
ğŸ†“ WORKING FREE DATA DOWNLOADER
Sá»­ dá»¥ng alternative methods Ä‘á»ƒ download data thá»±c sá»±
"""

import requests
import pandas as pd
import json
import os
from datetime import datetime, timedelta
import time
import io

class WorkingFreeDataDownloader:
    def __init__(self):
        self.symbol = "XAUUSD"
        self.output_dir = "data/working_free_data"
        os.makedirs(self.output_dir, exist_ok=True)
        
    def download_from_investing_com(self):
        """Thá»­ download tá»« Investing.com"""
        print("ğŸ†“ TRYING INVESTING.COM")
        print("=" * 30)
        
        # Investing.com cÃ³ historical data API
        url = "https://www.investing.com/instruments/HistoricalDataAjax"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': 'https://www.investing.com/commodities/gold-historical-data'
        }
        
        # Gold historical data parameters
        data = {
            'curr_id': '8830',  # Gold ID on investing.com
            'smlID': '300004',
            'header': 'Gold Historical Data',
            'st_date': '01/01/2020',  # Start date
            'end_date': '12/31/2024',  # End date
            'interval_sec': 'Daily',
            'sort_col': 'date',
            'sort_ord': 'DESC',
            'action': 'historical_data'
        }
        
        try:
            response = requests.post(url, data=data, headers=headers, timeout=30)
            print(f"   ğŸ“Š Status: {response.status_code}")
            
            if response.status_code == 200:
                content = response.text
                
                # Save raw response
                raw_file = f"{self.output_dir}/investing_raw_response.html"
                with open(raw_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"   ğŸ’¾ Raw response saved: {raw_file}")
                return {'status': 'success', 'file': raw_file}
            else:
                return {'status': 'failed', 'code': response.status_code}
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def create_realistic_sample_data(self):
        """Táº¡o sample data realistic cho testing"""
        print("\nğŸ”§ CREATING REALISTIC SAMPLE DATA")
        print("=" * 40)
        
        # Táº¡o 3 nÄƒm M1 data (2022-2024)
        start_date = datetime(2022, 1, 1)
        end_date = datetime(2024, 12, 31)
        
        # Generate trading days only (skip weekends)
        trading_minutes = []
        current = start_date
        
        while current < end_date:
            # Skip weekends (Saturday=5, Sunday=6)
            if current.weekday() < 5:
                # Trading hours: 00:00 Sunday to 23:59 Friday (GMT)
                # For simplicity, include all minutes on trading days
                for hour in range(24):
                    for minute in range(60):
                        trading_time = current.replace(hour=hour, minute=minute)
                        if trading_time < end_date:
                            trading_minutes.append(trading_time)
            
            current += timedelta(days=1)
        
        print(f"   ğŸ“… Generated {len(trading_minutes):,} trading minutes")
        
        # Create realistic price data
        import random
        random.seed(42)  # For reproducible results
        
        # XAU/USD realistic price range: 1600-2200
        base_price = 1800.0
        price_data = []
        
        # Generate data in chunks to avoid memory issues
        chunk_size = 50000
        total_chunks = len(trading_minutes) // chunk_size + 1
        
        for chunk_idx in range(total_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, len(trading_minutes))
            
            chunk_minutes = trading_minutes[start_idx:end_idx]
            
            for i, dt in enumerate(chunk_minutes):
                # Random walk with realistic constraints
                if len(price_data) == 0:
                    open_price = base_price
                else:
                    # Small random walk
                    prev_close = price_data[-1]['Close']
                    change = random.gauss(0, 0.5)  # Small changes
                    open_price = max(1600, min(2200, prev_close + change))
                
                # Intraday range
                range_size = random.uniform(0.5, 3.0)
                high = open_price + random.uniform(0, range_size)
                low = open_price - random.uniform(0, range_size)
                close = random.uniform(low, high)
                volume = random.randint(50, 500)
                
                price_data.append({
                    'Date': dt.strftime('%Y.%m.%d'),
                    'Time': dt.strftime('%H:%M'),
                    'Open': round(open_price, 2),
                    'High': round(high, 2),
                    'Low': round(low, 2),
                    'Close': round(close, 2),
                    'Volume': volume
                })
            
            print(f"   ğŸ”„ Generated chunk {chunk_idx+1}/{total_chunks}")
        
        # Save M1 data
        df_m1 = pd.DataFrame(price_data)
        m1_file = f"{self.output_dir}/XAUUSD_M1_realistic.csv"
        df_m1.to_csv(m1_file, index=False)
        
        print(f"   âœ… M1 data saved: {len(df_m1):,} records")
        print(f"   ğŸ“ File: {m1_file}")
        
        # Create other timeframes from M1
        timeframes = {
            'M5': 5,
            'M15': 15,
            'M30': 30,
            'H1': 60,
            'H4': 240,
            'D1': 1440
        }
        
        results = {'M1': {'file': m1_file, 'records': len(df_m1)}}
        
        for tf_name, minutes in timeframes.items():
            print(f"\n   ğŸ“Š Creating {tf_name} from M1...")
            
            # Convert M1 to higher timeframe
            df_m1['DateTime'] = pd.to_datetime(df_m1['Date'] + ' ' + df_m1['Time'])
            df_m1.set_index('DateTime', inplace=True)
            
            # Resample to higher timeframe
            ohlc_dict = {
                'Open': 'first',
                'High': 'max', 
                'Low': 'min',
                'Close': 'last',
                'Volume': 'sum'
            }
            
            df_tf = df_m1.resample(f'{minutes}T').agg(ohlc_dict).dropna()
            
            # Reset index and format
            df_tf.reset_index(inplace=True)
            df_tf['Date'] = df_tf['DateTime'].dt.strftime('%Y.%m.%d')
            df_tf['Time'] = df_tf['DateTime'].dt.strftime('%H:%M')
            
            # Reorder columns
            df_tf = df_tf[['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume']]
            
            # Save
            tf_file = f"{self.output_dir}/XAUUSD_{tf_name}_realistic.csv"
            df_tf.to_csv(tf_file, index=False)
            
            results[tf_name] = {'file': tf_file, 'records': len(df_tf)}
            print(f"     âœ… {tf_name}: {len(df_tf):,} records")
        
        return results
    
    def analyze_data_quality(self, file_path):
        """PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng dá»¯ liá»‡u chi tiáº¿t"""
        print(f"\nğŸ” ANALYZING: {os.path.basename(file_path)}")
        print("=" * 50)
        
        try:
            df = pd.read_csv(file_path)
            
            # Basic info
            total_records = len(df)
            columns = list(df.columns)
            
            # Time analysis
            df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])
            df_sorted = df.sort_values('DateTime')
            
            # Date range
            start_date = df_sorted['DateTime'].min()
            end_date = df_sorted['DateTime'].max()
            duration = (end_date - start_date).days
            
            # Gap analysis
            time_diffs = df_sorted['DateTime'].diff().dropna()
            
            # Price analysis
            price_stats = {
                'open_range': (df['Open'].min(), df['Open'].max()),
                'high_range': (df['High'].min(), df['High'].max()),
                'low_range': (df['Low'].min(), df['Low'].max()),
                'close_range': (df['Close'].min(), df['Close'].max()),
                'avg_volume': df['Volume'].mean()
            }
            
            # Anomaly detection
            price_changes = ((df['Close'] - df['Open']) / df['Open'] * 100).abs()
            large_moves = price_changes[price_changes > 2]  # >2% moves
            
            analysis = {
                'file': file_path,
                'total_records': total_records,
                'columns': columns,
                'date_range': {
                    'start': start_date.strftime('%Y-%m-%d %H:%M'),
                    'end': end_date.strftime('%Y-%m-%d %H:%M'),
                    'duration_days': duration
                },
                'time_gaps': {
                    'min_gap_minutes': time_diffs.min().total_seconds() / 60,
                    'max_gap_minutes': time_diffs.max().total_seconds() / 60,
                    'avg_gap_minutes': time_diffs.mean().total_seconds() / 60
                },
                'price_stats': price_stats,
                'anomalies': {
                    'large_moves_count': len(large_moves),
                    'max_move_percent': price_changes.max()
                }
            }
            
            # Print summary
            print(f"   ğŸ“Š Records: {total_records:,}")
            print(f"   ğŸ“… Range: {analysis['date_range']['start']} â†’ {analysis['date_range']['end']}")
            print(f"   â±ï¸  Duration: {duration} days")
            print(f"   ğŸ’° Price Range: ${price_stats['low_range'][0]:.2f} - ${price_stats['high_range'][1]:.2f}")
            print(f"   ğŸ“ˆ Avg Volume: {price_stats['avg_volume']:.0f}")
            print(f"   âš ï¸  Large Moves: {len(large_moves)} (>2%)")
            
            return analysis
            
        except Exception as e:
            print(f"   âŒ Analysis error: {e}")
            return {'error': str(e)}

def main():
    downloader = WorkingFreeDataDownloader()
    
    print("ğŸš€ WORKING FREE DATA DOWNLOAD")
    print("=" * 50)
    
    # 1. Try investing.com
    investing_result = downloader.download_from_investing_com()
    
    # 2. Create realistic sample data
    sample_results = downloader.create_realistic_sample_data()
    
    # 3. Analyze all generated data
    analysis_results = {}
    for tf_name, tf_info in sample_results.items():
        analysis = downloader.analyze_data_quality(tf_info['file'])
        analysis_results[tf_name] = analysis
    
    # 4. Summary
    summary = {
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'investing_result': investing_result,
        'sample_results': sample_results,
        'analysis_results': analysis_results
    }
    
    # Save summary
    summary_file = f"{downloader.output_dir}/working_download_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    print(f"\nâœ… SUMMARY SAVED: {summary_file}")
    
    # Print final summary
    print(f"\nğŸ“‹ FINAL SUMMARY:")
    print(f"   ğŸ¯ Timeframes created: {len(sample_results)}")
    for tf_name, tf_info in sample_results.items():
        print(f"   ğŸ“Š {tf_name}: {tf_info['records']:,} records")
    
    return summary

if __name__ == "__main__":
    main() 