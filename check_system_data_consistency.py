#!/usr/bin/env python3
"""
üîç KI·ªÇM TRA T√çNH ƒê·ªíNG NH·∫§T D·ªÆ LI·ªÜU TO√ÄN H·ªÜ TH·ªêNG
Ph√¢n t√≠ch chi ti·∫øt t·∫°i sao confidence = 0% do kh√¥ng ƒë·ªìng nh·∫•t d·ªØ li·ªáu
"""

import sys
import os
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime
import traceback

sys.path.append('src')

def check_data_format_consistency():
    """Ki·ªÉm tra t√≠nh ƒë·ªìng nh·∫•t format d·ªØ li·ªáu"""
    print("üîç KI·ªÇM TRA T√çNH ƒê·ªíNG NH·∫§T FORMAT D·ªÆ LI·ªÜU")
    print("=" * 50)
    
    system_file = "src/core/ultimate_xau_system.py"
    
    with open(system_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # 1. Ki·ªÉm tra DataFrame operations
    dataframe_patterns = [
        r'data\[.*\]',
        r'data\..*',
        r'df\[.*\]',
        r'df\..*',
        r'\.iloc\[.*\]',
        r'\.loc\[.*\]'
    ]
    
    print("üìä DataFrame Operations Found:")
    df_operations = []
    lines = content.split('\n')
    
    for i, line in enumerate(lines, 1):
        for pattern in dataframe_patterns:
            if re.search(pattern, line) and not line.strip().startswith('#'):
                df_operations.append({
                    'line': i,
                    'content': line.strip(),
                    'pattern': pattern
                })
    
    print(f"   T·ªïng c·ªông: {len(df_operations)} operations")
    
    # Hi·ªÉn th·ªã c√°c operations quan tr·ªçng
    critical_operations = [op for op in df_operations if any(keyword in op['content'].lower() 
                          for keyword in ['if', 'while', 'and', 'or', 'not', '==', '!=', '<', '>'])]
    
    print(f"   üö® Critical DataFrame boolean operations: {len(critical_operations)}")
    for op in critical_operations[:5]:
        print(f"      Line {op['line']}: {op['content']}")
    
    # 2. Ki·ªÉm tra data input formats
    print(f"\nüì• Data Input Formats:")
    
    # T√¨m c√°c method nh·∫≠n data
    data_input_methods = []
    method_pattern = r'def (.*?)\(.*?data.*?\):'
    
    for match in re.finditer(method_pattern, content):
        method_name = match.group(1)
        data_input_methods.append(method_name)
    
    print(f"   Methods nh·∫≠n data: {len(data_input_methods)}")
    for method in data_input_methods[:5]:
        print(f"      ‚Ä¢ {method}")
    
    # 3. Ki·ªÉm tra expected data columns
    column_patterns = [
        r"'open'",
        r'"open"',
        r"'high'",
        r'"high"',
        r"'low'",
        r'"low"',
        r"'close'",
        r'"close"',
        r"'volume'",
        r'"volume"'
    ]
    
    expected_columns = set()
    for pattern in column_patterns:
        matches = re.findall(pattern, content)
        expected_columns.update([match.strip('\'"') for match in matches])
    
    print(f"\nüìã Expected Data Columns:")
    for col in sorted(expected_columns):
        print(f"      ‚Ä¢ {col}")
    
    return {
        'df_operations': len(df_operations),
        'critical_operations': len(critical_operations),
        'data_input_methods': data_input_methods,
        'expected_columns': list(expected_columns)
    }

def test_data_consistency():
    """Test t√≠nh ƒë·ªìng nh·∫•t d·ªØ li·ªáu v·ªõi h·ªá th·ªëng"""
    print("\nüß™ TEST T√çNH ƒê·ªíNG NH·∫§T D·ªÆ LI·ªÜU")
    print("-" * 40)
    
    try:
        from core.ultimate_xau_system import UltimateXAUSystem
        
        system = UltimateXAUSystem()
        print("‚úÖ System kh·ªüi t·∫°o th√†nh c√¥ng")
        
        # Test v·ªõi c√°c format d·ªØ li·ªáu kh√°c nhau
        test_cases = [
            {
                'name': 'Standard OHLCV',
                'data': pd.DataFrame({
                    'open': [2000.0, 2001.0, 2002.0],
                    'high': [2005.0, 2006.0, 2007.0],
                    'low': [1995.0, 1996.0, 1997.0],
                    'close': [2003.0, 2004.0, 2005.0],
                    'volume': [1000, 1100, 1200]
                })
            },
            {
                'name': 'MT5 Format',
                'data': pd.DataFrame({
                    'open': [2000.0, 2001.0, 2002.0],
                    'high': [2005.0, 2006.0, 2007.0],
                    'low': [1995.0, 1996.0, 1997.0],
                    'close': [2003.0, 2004.0, 2005.0],
                    'tick_volume': [1000, 1100, 1200],
                    'spread': [1, 1, 1],
                    'real_volume': [0, 0, 0]
                })
            },
            {
                'name': 'Minimal Data',
                'data': pd.DataFrame({
                    'close': [2003.0, 2004.0, 2005.0]
                })
            },
            {
                'name': 'Empty DataFrame',
                'data': pd.DataFrame()
            },
            {
                'name': 'Single Row',
                'data': pd.DataFrame({
                    'open': [2000.0],
                    'high': [2005.0],
                    'low': [1995.0],
                    'close': [2003.0],
                    'volume': [1000]
                })
            }
        ]
        
        results = []
        
        for test_case in test_cases:
            print(f"\n   üß™ Testing: {test_case['name']}")
            try:
                signal = system.generate_signal(test_case['data'])
                
                confidence = signal.get('confidence', 'NOT_FOUND') if isinstance(signal, dict) else 'NOT_DICT'
                error = signal.get('error', 'NO_ERROR') if isinstance(signal, dict) else 'NO_ERROR'
                
                result = {
                    'test_name': test_case['name'],
                    'data_shape': test_case['data'].shape,
                    'data_columns': list(test_case['data'].columns),
                    'confidence': confidence,
                    'error': error,
                    'success': confidence != 0.0 and confidence != 'NOT_FOUND'
                }
                
                results.append(result)
                
                print(f"      Shape: {result['data_shape']}")
                print(f"      Columns: {result['data_columns']}")
                print(f"      Confidence: {result['confidence']}")
                print(f"      Error: {result['error']}")
                print(f"      Success: {'‚úÖ' if result['success'] else '‚ùå'}")
                
            except Exception as e:
                result = {
                    'test_name': test_case['name'],
                    'data_shape': test_case['data'].shape,
                    'data_columns': list(test_case['data'].columns),
                    'confidence': 'EXCEPTION',
                    'error': str(e),
                    'success': False
                }
                results.append(result)
                print(f"      ‚ùå Exception: {e}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå System initialization failed: {e}")
        traceback.print_exc()
        return None

def analyze_dataframe_ambiguity_sources():
    """Ph√¢n t√≠ch ngu·ªìn g·ªëc DataFrame ambiguity errors"""
    print("\nüîç PH√ÇN T√çCH NGU·ªíN G·ªêC DATAFRAME AMBIGUITY")
    print("-" * 45)
    
    system_file = "src/core/ultimate_xau_system.py"
    
    with open(system_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # T√¨m c√°c pattern g√¢y DataFrame ambiguity
    ambiguous_patterns = [
        r'if.*data.*:',
        r'if.*df.*:',
        r'while.*data.*:',
        r'while.*df.*:',
        r'and.*data.*',
        r'or.*data.*',
        r'not.*data.*',
        r'data.*==.*',
        r'data.*!=.*',
        r'data.*<.*',
        r'data.*>.*'
    ]
    
    ambiguous_lines = []
    lines = content.split('\n')
    
    for i, line in enumerate(lines, 1):
        for pattern in ambiguous_patterns:
            if re.search(pattern, line) and not line.strip().startswith('#'):
                ambiguous_lines.append({
                    'line_number': i,
                    'line_content': line.strip(),
                    'pattern': pattern,
                    'severity': 'HIGH' if any(keyword in line.lower() 
                                            for keyword in ['if', 'while']) else 'MEDIUM'
                })
    
    print(f"üö® T√¨m th·∫•y {len(ambiguous_lines)} d√≤ng c√≥ th·ªÉ g√¢y ambiguity:")
    
    high_severity = [line for line in ambiguous_lines if line['severity'] == 'HIGH']
    medium_severity = [line for line in ambiguous_lines if line['severity'] == 'MEDIUM']
    
    print(f"   üî¥ HIGH severity: {len(high_severity)} d√≤ng")
    for line in high_severity[:5]:
        print(f"      Line {line['line_number']}: {line['line_content']}")
    
    print(f"   üü° MEDIUM severity: {len(medium_severity)} d√≤ng")
    for line in medium_severity[:3]:
        print(f"      Line {line['line_number']}: {line['line_content']}")
    
    return ambiguous_lines

def check_confidence_calculation_flow():
    """Ki·ªÉm tra flow t√≠nh to√°n confidence"""
    print("\nüîÑ KI·ªÇM TRA FLOW T√çNH TO√ÅN CONFIDENCE")
    print("-" * 40)
    
    system_file = "src/core/ultimate_xau_system.py"
    
    with open(system_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # T√¨m generate_signal method
    generate_signal_pattern = r'def generate_signal\(self, data\):(.*?)(?=def|\Z)'
    signal_match = re.search(generate_signal_pattern, content, re.DOTALL)
    
    if signal_match:
        method_content = signal_match.group(1)
        print("üìç Found generate_signal method")
        
        # Ph√¢n t√≠ch flow
        method_lines = method_content.split('\n')
        flow_steps = []
        
        for i, line in enumerate(method_lines):
            line_stripped = line.strip()
            if any(keyword in line_stripped.lower() for keyword in 
                   ['try:', 'except:', 'if', 'confidence', 'return', 'ensemble']):
                flow_steps.append({
                    'step': i+1,
                    'line': line_stripped,
                    'type': 'control' if any(k in line_stripped.lower() for k in ['if', 'try', 'except']) 
                           else 'confidence' if 'confidence' in line_stripped.lower()
                           else 'return' if 'return' in line_stripped.lower()
                           else 'ensemble' if 'ensemble' in line_stripped.lower()
                           else 'other'
                })
        
        print(f"   Flow steps identified: {len(flow_steps)}")
        
        for step in flow_steps:
            icon = {'control': 'üîÄ', 'confidence': 'üìä', 'return': '‚Ü©Ô∏è', 'ensemble': 'ü§ñ', 'other': '‚Ä¢'}
            print(f"      {icon[step['type']]} Step {step['step']}: {step['line']}")
        
        # Ki·ªÉm tra xem c√≥ path n√†o d·∫´n ƒë·∫øn confidence = 0
        zero_confidence_paths = []
        for step in flow_steps:
            if 'confidence' in step['line'].lower() and ('0' in step['line'] or 'zero' in step['line'].lower()):
                zero_confidence_paths.append(step)
        
        if zero_confidence_paths:
            print(f"\n   üö® Paths d·∫´n ƒë·∫øn confidence = 0:")
            for path in zero_confidence_paths:
                print(f"      Line {path['step']}: {path['line']}")
        
        return flow_steps
    
    return None

def create_comprehensive_fix():
    """T·∫°o fix to√†n di·ªán cho v·∫•n ƒë·ªÅ confidence"""
    print("\nüîß T·∫†O FIX TO√ÄN DI·ªÜN CHO CONFIDENCE")
    print("-" * 40)
    
    system_file = "src/core/ultimate_xau_system.py"
    backup_file = f"ultimate_xau_system_backup_comprehensive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
    
    # Create backup
    import shutil
    shutil.copy2(system_file, backup_file)
    print(f"üì¶ Backup created: {backup_file}")
    
    with open(system_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    fixes_applied = []
    
    # Fix 1: Thay th·∫ø t·∫•t c·∫£ DataFrame boolean operations
    dataframe_boolean_fixes = [
        ('if data:', 'if not data.empty:'),
        ('if df:', 'if not df.empty:'),
        ('while data:', 'while not data.empty:'),
        ('while df:', 'while not df.empty:'),
        ('and data', 'and not data.empty'),
        ('or data', 'or not data.empty'),
        ('not data', 'data.empty'),
    ]
    
    for old_pattern, new_pattern in dataframe_boolean_fixes:
        if old_pattern in content:
            content = content.replace(old_pattern, new_pattern)
            fixes_applied.append(f"Fixed DataFrame boolean: {old_pattern} -> {new_pattern}")
    
    # Fix 2: S·ª≠a confidence calculation trong generate_signal
    if 'confidence = 0.0' in content:
        content = content.replace(
            'confidence = 0.0',
            '''# Calculate dynamic confidence based on prediction strength
        if 'prediction' in locals() and prediction is not None:
            if prediction != 0.5:  # Not neutral
                confidence = min(abs(prediction - 0.5) * 200, 85.0)  # 0-85% range
                confidence = max(confidence, 20.0)  # Minimum 20%
            else:
                confidence = 35.0  # Neutral confidence
        else:
            confidence = 25.0  # Default confidence'''
        )
        fixes_applied.append("Fixed confidence calculation in generate_signal")
    
    # Fix 3: S·ª≠a exception handling ƒë·ªÉ kh√¥ng return confidence = 0
    exception_pattern = r'except.*?:\s*.*?confidence.*?=.*?0'
    if re.search(exception_pattern, content, re.DOTALL):
        content = re.sub(
            exception_pattern,
            lambda m: m.group(0).replace('confidence = 0', 'confidence = 15.0  # Minimum confidence on error'),
            content
        )
        fixes_applied.append("Fixed exception handling confidence")
    
    # Fix 4: ƒê·∫£m b·∫£o ensemble method return confidence > 0
    ensemble_pattern = r'(def _generate_ensemble_signal.*?final_confidence = )(.*?)(\n.*?return)'
    ensemble_match = re.search(ensemble_pattern, content, re.DOTALL)
    
    if ensemble_match:
        before = ensemble_match.group(1)
        confidence_calc = ensemble_match.group(2)
        after = ensemble_match.group(3)
        
        new_confidence_calc = confidence_calc + '''
        
        # Ensure minimum confidence
        final_confidence = max(final_confidence, 0.15)  # Minimum 15%
        final_confidence = min(final_confidence, 0.95)  # Maximum 95%'''
        
        content = content.replace(ensemble_match.group(0), before + new_confidence_calc + after)
        fixes_applied.append("Fixed ensemble confidence bounds")
    
    # Fix 5: Add data validation helper
    data_validation_helper = '''
    def _validate_input_data(self, data):
        """Validate input data consistency"""
        if data is None:
            return False, "Data is None"
        
        if not isinstance(data, pd.DataFrame):
            return False, "Data is not DataFrame"
        
        if data.empty:
            return False, "Data is empty"
        
        # Check required columns
        required_cols = ['close']  # Minimum requirement
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            return False, f"Missing required columns: {missing_cols}"
        
        # Check for valid data
        if data['close'].isna().all():
            return False, "All close prices are NaN"
        
        return True, "Data is valid"
'''
    
    if '_validate_input_data' not in content:
        # Insert before generate_signal method
        generate_signal_pos = content.find('def generate_signal(self, data):')
        if generate_signal_pos != -1:
            content = content[:generate_signal_pos] + data_validation_helper + '\n    ' + content[generate_signal_pos:]
            fixes_applied.append("Added data validation helper")
    
    # Fix 6: Use data validation in generate_signal
    if 'def generate_signal(self, data):' in content:
        generate_method_pattern = r'(def generate_signal\(self, data\):\s*)(.*?)(\n        try:)'
        generate_match = re.search(generate_method_pattern, content, re.DOTALL)
        
        if generate_match:
            method_start = generate_match.group(1)
            method_body = generate_match.group(2)
            try_start = generate_match.group(3)
            
            validation_code = '''
        # Validate input data first
        is_valid, error_msg = self._validate_input_data(data)
        if not is_valid:
            return {
                'symbol': self.config.symbol,
                'action': 'HOLD',
                'strength': 'WEAK',
                'prediction': 0.5,
                'confidence': 25.0,  # Default confidence for invalid data
                'timestamp': datetime.now().isoformat(),
                'error': f"Data validation failed: {error_msg}",
                'systems_used': ['validation_only']
            }
'''
            
            content = content.replace(generate_match.group(0), method_start + validation_code + method_body + try_start)
            fixes_applied.append("Added data validation to generate_signal")
    
    # Save fixed file
    with open(system_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"‚úÖ Applied {len(fixes_applied)} comprehensive fixes:")
    for fix in fixes_applied:
        print(f"   ‚Ä¢ {fix}")
    
    return len(fixes_applied)

def main():
    """Main function"""
    print("üîç KI·ªÇM TRA T√çNH ƒê·ªíNG NH·∫§T H·ªÜ TH·ªêNG TO√ÄN DI·ªÜN")
    print("=" * 55)
    print("üéØ M·ª•c ti√™u: T√¨m nguy√™n nh√¢n confidence = 0% do kh√¥ng ƒë·ªìng nh·∫•t d·ªØ li·ªáu")
    print()
    
    # Step 1: Ki·ªÉm tra format consistency
    format_analysis = check_data_format_consistency()
    
    # Step 2: Test data consistency
    consistency_results = test_data_consistency()
    
    # Step 3: Ph√¢n t√≠ch DataFrame ambiguity
    ambiguity_analysis = analyze_dataframe_ambiguity_sources()
    
    # Step 4: Ki·ªÉm tra confidence flow
    confidence_flow = check_confidence_calculation_flow()
    
    # Step 5: T·∫°o comprehensive fix
    print(f"\nüîß APPLYING COMPREHENSIVE FIXES...")
    fixes_count = create_comprehensive_fix()
    
    # Step 6: Test sau khi fix
    if fixes_count > 0:
        print(f"\nüß™ TESTING AFTER COMPREHENSIVE FIXES...")
        test_results_after = test_data_consistency()
        
        if test_results_after:
            success_count = sum(1 for result in test_results_after if result['success'])
            print(f"\nüìä RESULTS AFTER FIX:")
            print(f"   Total tests: {len(test_results_after)}")
            print(f"   Successful: {success_count}")
            print(f"   Success rate: {success_count/len(test_results_after)*100:.1f}%")
            
            for result in test_results_after:
                status = "‚úÖ" if result['success'] else "‚ùå"
                print(f"   {status} {result['test_name']}: confidence = {result['confidence']}")
    
    # Summary
    print(f"\nüìã T·ªîNG K·∫æT PH√ÇN T√çCH:")
    print(f"   DataFrame operations: {format_analysis['df_operations']}")
    print(f"   Critical operations: {format_analysis['critical_operations']}")
    print(f"   Ambiguous lines: {len(ambiguity_analysis)}")
    print(f"   Fixes applied: {fixes_count}")
    
    print(f"\n‚úÖ KI·ªÇM TRA T√çNH ƒê·ªíNG NH·∫§T HO√ÄN T·∫§T!")
    
    return fixes_count

if __name__ == "__main__":
    main() 