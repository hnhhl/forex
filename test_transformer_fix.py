#!/usr/bin/env python3
"""
üß™ TEST TRANSFORMER FIX
Test transformer models sau khi fix architecture
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def test_transformer_fix():
    print("üß™ TESTING TRANSFORMER FIX")
    print("=" * 50)
    
    # Import t·ª´ file ƒë√£ s·ª≠a
    sys.path.append('.')
    from ULTIMATE_GROUP_TRAINING_SYSTEM import GroupTrainingOrchestrator, GroupTrainingConfig
    
    # Test 1: T·∫°o transformer model
    print("1. TESTING TRANSFORMER MODEL CREATION")
    print("-" * 40)
    
    try:
        config = GroupTrainingConfig()
        orchestrator = GroupTrainingOrchestrator(config)
        
        # T·∫°o transformer spec
        transformer_spec = {
            'model_id': 'test_transformer_001',
            'type': 'transformer',
            'architecture': 'transformer',
            'n_heads': 4,
            'n_layers': 2
        }
        
        input_size = 20  # 20 features
        model = orchestrator._create_model(transformer_spec, input_size)
        
        print(f"‚úÖ Transformer model created successfully")
        print(f"   Input size: {input_size}")
        print(f"   Model: {type(model).__name__}")
        
    except Exception as e:
        print(f"‚ùå Model creation failed: {e}")
        return False
    
    # Test 2: Test forward pass
    print("\n2. TESTING FORWARD PASS")
    print("-" * 40)
    
    try:
        # Test data
        batch_size = 32
        test_input = torch.randn(batch_size, input_size)
        
        # Force CPU for transformer
        model = model.to('cpu')
        test_input = test_input.to('cpu')
        
        # Forward pass
        model.eval()
        with torch.no_grad():
            output = model(test_input)
        
        print(f"‚úÖ Forward pass successful")
        print(f"   Input shape: {test_input.shape}")
        print(f"   Output shape: {output.shape}")
        print(f"   Output range: [{output.min().item():.4f}, {output.max().item():.4f}]")
        
        # Check output validity
        if output.shape == (batch_size, 1) and 0 <= output.min() <= output.max() <= 1:
            print(f"‚úÖ Output format is valid (sigmoid output)")
        else:
            print(f"‚ùå Output format invalid")
            return False
            
    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
        return False
    
    # Test 3: Test training step
    print("\n3. TESTING TRAINING STEP")
    print("-" * 40)
    
    try:
        # Training setup
        model.train()
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # Sample training data
        X_train = torch.randn(batch_size, input_size)
        y_train = torch.randint(0, 2, (batch_size,)).float()
        
        # Training step
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs.squeeze(), y_train)
        loss.backward()
        optimizer.step()
        
        print(f"‚úÖ Training step successful")
        print(f"   Loss: {loss.item():.4f}")
        print(f"   Gradients computed successfully")
        
    except Exception as e:
        print(f"‚ùå Training step failed: {e}")
        return False
    
    # Test 4: Test device compatibility
    print("\n4. TESTING DEVICE COMPATIBILITY")
    print("-" * 40)
    
    try:
        # Test CPU
        model_cpu = model.to('cpu')
        input_cpu = torch.randn(10, input_size).to('cpu')
        output_cpu = model_cpu(input_cpu)
        print(f"‚úÖ CPU compatibility: OK")
        
        # Test GPU if available
        if torch.cuda.is_available():
            try:
                model_gpu = model.to('cuda')
                input_gpu = torch.randn(10, input_size).to('cuda')
                output_gpu = model_gpu(input_gpu)
                print(f"‚úÖ GPU compatibility: OK")
            except Exception as e:
                print(f"‚ö†Ô∏è GPU test failed (expected for transformer): {e}")
                print(f"‚úÖ This is why we use CPU for transformers")
        else:
            print(f"‚ö†Ô∏è CUDA not available, skipping GPU test")
            
    except Exception as e:
        print(f"‚ùå Device compatibility test failed: {e}")
        return False
    
    # Test 5: Test multiple transformer models
    print("\n5. TESTING MULTIPLE TRANSFORMER MODELS")
    print("-" * 40)
    
    try:
        transformer_specs = [
            {'model_id': f'transformer_{i:03d}', 'type': 'transformer', 'architecture': 'transformer', 'n_heads': 4}
            for i in range(1, 6)  # Test 5 transformers
        ]
        
        successful_models = 0
        for spec in transformer_specs:
            try:
                model = orchestrator._create_model(spec, input_size).to('cpu')
                test_input = torch.randn(5, input_size).to('cpu')
                output = model(test_input)
                successful_models += 1
                print(f"   ‚úÖ {spec['model_id']}: OK")
            except Exception as e:
                print(f"   ‚ùå {spec['model_id']}: {e}")
        
        print(f"‚úÖ Multiple models test: {successful_models}/5 successful")
        
        if successful_models >= 4:  # At least 80% success
            print(f"‚úÖ Transformer fix is working well")
        else:
            print(f"‚ö†Ô∏è Some transformers still failing")
            
    except Exception as e:
        print(f"‚ùå Multiple models test failed: {e}")
        return False
    
    print("\n" + "=" * 50)
    print("üéä TRANSFORMER FIX TEST COMPLETED!")
    print("‚úÖ Transformer architecture fixed")
    print("‚úÖ CPU compatibility confirmed")
    print("‚úÖ Ready for full training")
    print("=" * 50)
    
    return True

def test_integration_with_group_training():
    """Test integration v·ªõi group training system"""
    print("\nüîß TESTING INTEGRATION WITH GROUP TRAINING")
    print("-" * 50)
    
    try:
        from ULTIMATE_GROUP_TRAINING_SYSTEM import GroupTrainingConfig
        
        config = GroupTrainingConfig()
        
        # Test config
        print(f"‚úÖ Config loaded:")
        print(f"   Transformer parallel: {config.transformer_group_parallel}")
        print(f"   Neural epochs: {config.neural_epochs}")
        print(f"   Batch size: {config.neural_batch_size}")
        
        print(f"‚úÖ Ready for full Group Training with transformer fix")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ TRANSFORMER FIX TEST SUITE")
    print("=" * 60)
    
    # Test transformer fix
    transformer_success = test_transformer_fix()
    
    if transformer_success:
        # Test integration
        integration_success = test_integration_with_group_training()
        
        if integration_success:
            print("\nüéâ ALL TESTS PASSED!")
            print("üî• Transformer models are now ready for training!")
            print("üöÄ Run full Group Training to get all 250 models working!")
        else:
            print("\n‚ùå Integration tests failed")
    else:
        print("\n‚ùå Transformer fix tests failed") 