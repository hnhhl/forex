#!/usr/bin/env python3
"""
ðŸš€ ENHANCED XAU SYSTEM V4.0 WITH AI2.0 INTEGRATION
======================================================================
ðŸŽ¯ TÃ­ch há»£p AI2.0 Trading Strategy (11,960 trades, 83.17% win rate)
ðŸ“ˆ Enhanced Unified Multi-Timeframe System vá»›i AI2.0 Voting
ðŸ’° Target: Cáº£i thiá»‡n performance tá»« 86.46% lÃªn 90%+
"""

import pandas as pd
import numpy as np
import json
import os
import pickle
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb

class EnhancedXAUSystemV4WithAI2:
    def __init__(self):
        self.data_dir = "data/working_free_data"
        self.results_dir = "enhanced_v4_ai2_results"
        
        os.makedirs(self.results_dir, exist_ok=True)
        
        self.ai2_params = {
            'step_size': 30,
            'lookback_candles': 20,
            'future_lookahead': 15
        }
        
        self.multi_timeframe_data = {}
        self.unified_dataset = None
        self.models = {}
        self.training_results = {}
        
        print("ðŸš€ ENHANCED XAU SYSTEM V4.0 WITH AI2.0 INTEGRATION")
    
    def load_data(self):
        """Load multi-timeframe data"""
        print("ðŸ“Š LOADING DATA...")
        
        timeframes = ['M15']  # Focus on M15 for AI2.0
        
        for tf in timeframes:
            csv_file = f"{self.data_dir}/XAUUSD_{tf}_realistic.csv"
            
            if os.path.exists(csv_file):
                df = pd.read_csv(csv_file)
                df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])
                df = df.rename(columns={
                    'Open': 'open', 'High': 'high', 'Low': 'low', 
                    'Close': 'close', 'Volume': 'volume'
                })
                df = df.sort_values('datetime').reset_index(drop=True)
                self.multi_timeframe_data[tf] = df
                
                print(f"âœ… {tf}: {len(df):,} records")
        
        return len(self.multi_timeframe_data) > 0
    
    def create_features(self, df):
        """Create technical features"""
        print("ðŸ”§ Creating features...")
        
        # Moving averages
        df['sma_5'] = df['close'].rolling(5).mean()
        df['sma_10'] = df['close'].rolling(10).mean()
        df['sma_20'] = df['close'].rolling(20).mean()
        df['ema_10'] = df['close'].ewm(span=10).mean()
        
        # Technical indicators
        df['rsi'] = self.calculate_rsi(df['close'])
        df['macd'], df['macd_signal'] = self.calculate_macd(df['close'])
        
        # Volatility
        df['volatility'] = df['close'].pct_change().rolling(20).std()
        
        # Momentum
        df['momentum_5'] = df['close'] / df['close'].shift(5) - 1
        df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
        
        # Time features
        df['hour'] = df['datetime'].dt.hour
        df['day_of_week'] = df['datetime'].dt.dayofweek
        
        return df
    
    def generate_ai2_signals(self, df):
        """Generate AI2.0 voting signals"""
        print("ðŸ¤– Generating AI2.0 signals...")
        
        signals = []
        step_size = self.ai2_params['step_size']
        lookback = self.ai2_params['lookback_candles']
        future_lookahead = self.ai2_params['future_lookahead']
        
        for i in range(lookback, len(df) - future_lookahead, step_size):
            try:
                current_price = df.iloc[i]['close']
                recent_data = df.iloc[i-lookback:i+1]
                
                # Future price
                future_idx = min(i + future_lookahead, len(df) - 1)
                future_price = df.iloc[future_idx]['close']
                price_change_pct = (future_price - current_price) / current_price * 100
                
                # AI2.0 Voting System
                votes = []
                
                # Voter 1: Price Momentum
                if price_change_pct > 0.1:
                    votes.append('BUY')
                elif price_change_pct < -0.1:
                    votes.append('SELL')
                else:
                    votes.append('HOLD')
                
                # Voter 2: Technical Analysis
                sma_5 = recent_data['close'].rolling(5).mean().iloc[-1]
                sma_10 = recent_data['close'].rolling(10).mean().iloc[-1]
                
                if pd.notna(sma_5) and pd.notna(sma_10):
                    if current_price > sma_5 > sma_10:
                        votes.append('BUY')
                    elif current_price < sma_5 < sma_10:
                        votes.append('SELL')
                    else:
                        votes.append('HOLD')
                else:
                    votes.append('HOLD')
                
                # Voter 3: Volatility-Adjusted
                returns = recent_data['close'].pct_change().dropna()
                volatility = returns.std() * 100 if len(returns) > 1 else 0.5
                vol_threshold = max(0.05, volatility * 0.3)
                
                if price_change_pct > vol_threshold:
                    votes.append('BUY')
                elif price_change_pct < -vol_threshold:
                    votes.append('SELL')
                else:
                    votes.append('HOLD')
                
                # Count votes
                buy_votes = votes.count('BUY')
                sell_votes = votes.count('SELL')
                
                # Majority decision
                if buy_votes > sell_votes and buy_votes > 1:
                    signal = 1  # BUY
                elif sell_votes > buy_votes and sell_votes > 1:
                    signal = 0  # SELL
                else:
                    signal = 2  # HOLD
                
                signals.append(signal)
                
            except Exception:
                signals.append(2)  # Default HOLD
        
        # Signal distribution
        unique, counts = np.unique(signals, return_counts=True)
        for signal, count in zip(unique, counts):
            signal_name = ['SELL', 'BUY', 'HOLD'][signal]
            print(f"   {signal_name}: {count:,} ({count/len(signals)*100:.1f}%)")
        
        return signals
    
    def create_dataset(self):
        """Create unified dataset"""
        print("\nðŸ”„ CREATING DATASET...")
        
        if not self.multi_timeframe_data:
            return False
        
        base_data = self.multi_timeframe_data['M15'].copy()
        base_data = self.create_features(base_data)
        
        # Generate signals
        ai2_signals = self.generate_ai2_signals(base_data)
        
        # Feature columns
        feature_columns = [
            'sma_5', 'sma_10', 'sma_20', 'ema_10',
            'rsi', 'macd', 'macd_signal',
            'volatility', 'momentum_5', 'momentum_10',
            'hour', 'day_of_week'
        ]
        
        # Align features with signals
        step_size = self.ai2_params['step_size']
        lookback = self.ai2_params['lookback_candles']
        future_lookahead = self.ai2_params['future_lookahead']
        
        signal_indices = list(range(lookback, len(base_data) - future_lookahead, step_size))
        aligned_features = base_data.iloc[signal_indices][feature_columns].copy()
        
        # Ensure same length
        min_length = min(len(aligned_features), len(ai2_signals))
        aligned_features = aligned_features.iloc[:min_length]
        ai2_signals = ai2_signals[:min_length]
        
        # Remove NaN
        mask = ~aligned_features.isnull().any(axis=1)
        aligned_features = aligned_features[mask]
        ai2_signals = np.array(ai2_signals)[mask]
        
        self.unified_dataset = {
            'features': aligned_features,
            'labels': ai2_signals,
            'feature_columns': feature_columns
        }
        
        print(f"âœ… Dataset: {len(ai2_signals):,} samples, {len(feature_columns)} features")
        return True
    
    def train_models(self):
        """Train enhanced models"""
        print("\nðŸš€ TRAINING MODELS...")
        
        if self.unified_dataset is None:
            return False
        
        X = self.unified_dataset['features'].values
        y = self.unified_dataset['labels']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"Training: {len(X_train):,}, Test: {len(X_test):,}")
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        models_results = {}
        
        # Random Forest
        print("\nðŸŒ² Random Forest...")
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        rf_accuracy = accuracy_score(y_test, rf_pred)
        
        models_results['random_forest'] = {
            'accuracy': rf_accuracy,
            'predictions': rf_pred
        }
        print(f"   Accuracy: {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)")
        
        # Gradient Boosting
        print("\nâš¡ Gradient Boosting...")
        gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
        gb_model.fit(X_train, y_train)
        gb_pred = gb_model.predict(X_test)
        gb_accuracy = accuracy_score(y_test, gb_pred)
        
        models_results['gradient_boosting'] = {
            'accuracy': gb_accuracy,
            'predictions': gb_pred
        }
        print(f"   Accuracy: {gb_accuracy:.4f} ({gb_accuracy*100:.2f}%)")
        
        # LightGBM
        print("\nðŸ’¡ LightGBM...")
        lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
        lgb_model.fit(X_train, y_train)
        lgb_pred = lgb_model.predict(X_test)
        lgb_accuracy = accuracy_score(y_test, lgb_pred)
        
        models_results['lightgbm'] = {
            'accuracy': lgb_accuracy,
            'predictions': lgb_pred
        }
        print(f"   Accuracy: {lgb_accuracy:.4f} ({lgb_accuracy*100:.2f}%)")
        
        # Neural Network
        print("\nðŸ§  Neural Network...")
        nn_model = MLPClassifier(
            hidden_layer_sizes=(64, 32),
            max_iter=200,
            random_state=42
        )
        nn_model.fit(X_train_scaled, y_train)
        nn_pred = nn_model.predict(X_test_scaled)
        nn_accuracy = accuracy_score(y_test, nn_pred)
        
        models_results['neural_network'] = {
            'accuracy': nn_accuracy,
            'predictions': nn_pred
        }
        print(f"   Accuracy: {nn_accuracy:.4f} ({nn_accuracy*100:.2f}%)")
        
        # Ensemble
        print("\nðŸ¤ Ensemble...")
        ensemble_pred = []
        for i in range(len(y_test)):
            votes = [rf_pred[i], gb_pred[i], lgb_pred[i], nn_pred[i]]
            ensemble_pred.append(max(set(votes), key=votes.count))
        
        ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
        models_results['ensemble'] = {
            'accuracy': ensemble_accuracy,
            'predictions': ensemble_pred
        }
        print(f"   Accuracy: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)")
        
        self.training_results = {
            'models_results': models_results,
            'test_data': {'y_test': y_test},
            'dataset_info': {
                'total_samples': len(X),
                'features_count': X.shape[1],
                'train_samples': len(X_train),
                'test_samples': len(X_test)
            }
        }
        
        return True
    
    def simulate_trading(self):
        """Simulate trading performance"""
        print("\nðŸ’° TRADING SIMULATION...")
        
        trading_results = {}
        
        for model_name, result in self.training_results['models_results'].items():
            predictions = result['predictions']
            y_test = self.training_results['test_data']['y_test']
            
            # Simulation parameters
            initial_balance = 10000
            position_size_pct = 0.02
            balance = initial_balance
            trades = []
            
            for pred, actual in zip(predictions, y_test):
                if pred != 2:  # Not HOLD
                    position_value = balance * position_size_pct
                    
                    # Simulate price change
                    if pred == 1:  # BUY
                        price_change = np.random.normal(0.001, 0.01)
                    else:  # SELL
                        price_change = -np.random.normal(0.001, 0.01)
                    
                    pnl = position_value * price_change
                    balance += pnl
                    
                    trades.append({
                        'prediction': pred,
                        'actual': actual,
                        'pnl': pnl,
                        'balance': balance,
                        'correct': (pred == actual)
                    })
            
            if trades:
                trades_df = pd.DataFrame(trades)
                total_trades = len(trades)
                winning_trades = len(trades_df[trades_df['pnl'] > 0])
                win_rate = winning_trades / total_trades * 100
                total_return = (balance - initial_balance) / initial_balance * 100
                
                trading_results[model_name] = {
                    'total_trades': total_trades,
                    'win_rate': win_rate,
                    'total_return': total_return,
                    'final_balance': balance,
                    'model_accuracy': result['accuracy'] * 100
                }
                
                print(f"\nðŸ“Š {model_name.title()}:")
                print(f"   Trades: {total_trades}")
                print(f"   Win Rate: {win_rate:.1f}%")
                print(f"   Return: {total_return:+.1f}%")
        
        return trading_results
    
    def create_report(self):
        """Create comprehensive report"""
        print("\nðŸ“‹ CREATING REPORT...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        trading_results = self.simulate_trading()
        
        # Find best performers
        best_model = max(
            self.training_results['models_results'].items(),
            key=lambda x: x[1]['accuracy']
        )
        
        best_trading = max(
            trading_results.items(),
            key=lambda x: x[1]['total_return']
        )
        
        report = {
            'timestamp': timestamp,
            'system_info': {
                'version': 'Enhanced XAU System V4.0 with AI2.0',
                'base_timeframe': 'M15',
                'ai2_integration': True
            },
            'dataset_summary': self.training_results['dataset_info'],
            'model_performance': {
                name: {
                    'accuracy': result['accuracy'],
                    'accuracy_pct': result['accuracy'] * 100,
                    'improvement_vs_v4': (result['accuracy'] - 0.8646) * 100
                }
                for name, result in self.training_results['models_results'].items()
            },
            'trading_simulation': trading_results,
            'best_performers': {
                'best_model': {
                    'name': best_model[0],
                    'accuracy': best_model[1]['accuracy'] * 100
                },
                'best_trading': {
                    'model': best_trading[0],
                    'return': best_trading[1]['total_return']
                }
            },
            'vs_original_ai2': {
                'original_trades': 11960,
                'original_win_rate': 83.17,
                'original_return': 140.29
            }
        }
        
        # Save report
        report_file = f"{self.results_dir}/evaluation_report_{timestamp}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nðŸŽ¯ RESULTS SUMMARY:")
        print(f"Best Model: {best_model[0]} ({best_model[1]['accuracy']*100:.2f}%)")
        print(f"Best Trading: {best_trading[0]} ({best_trading[1]['total_return']:+.1f}%)")
        print(f"Report saved: {report_file}")
        
        return report_file
    
    def run_system(self):
        """Run complete system"""
        print("ðŸš€ RUNNING ENHANCED XAU SYSTEM V4.0 WITH AI2.0")
        print("=" * 60)
        
        if not self.load_data():
            return None
        
        if not self.create_dataset():
            return None
        
        if not self.train_models():
            return None
        
        return self.create_report()
    
    # Helper methods
    def calculate_rsi(self, prices, period=14):
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def calculate_macd(self, prices, fast=12, slow=26, signal=9):
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        macd_signal = macd.ewm(span=signal).mean()
        return macd, macd_signal

def main():
    system = EnhancedXAUSystemV4WithAI2()
    return system.run_system()

if __name__ == "__main__":
    main() 